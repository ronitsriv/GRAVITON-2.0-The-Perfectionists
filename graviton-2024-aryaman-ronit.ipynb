{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4080187,"sourceType":"datasetVersion","datasetId":2414370},{"sourceId":6786780,"sourceType":"datasetVersion","datasetId":3901070}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing necessary libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, VotingClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load the dataset\ndata = pd.read_csv(\"/kaggle/input/australia-weather-data/Weather Training Data.csv\")  # Replace with the actual file path\n\n# Data preprocessing\n# One-hot encode categorical variables\ndata = pd.get_dummies(data, columns=['WindGustDir', 'WindDir9am', 'WindDir3pm'], drop_first=True)\n\n# Encode target variable\nle = LabelEncoder()\ndata['RainToday'] = le.fit_transform(data['RainToday'])\ndata['RainTomorrow'] = le.fit_transform(data['RainTomorrow'])\n\n# Selecting features and target variable\nX = data.drop(columns=['Location', 'RainTomorrow'])  # Drop columns not needed for prediction\ny = data['RainTomorrow']\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define individual models\nmlp_clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\nxgboost_clf = XGBClassifier(random_state=42)\nada_boost_clf = AdaBoostClassifier(random_state=42)\nlgbm_clf = LGBMClassifier(random_state=42)\n\n# Define the ensemble model using VotingClassifier\nensemble_model = VotingClassifier(estimators=[('mlp', mlp_clf),\n                                              ('xgboost', xgboost_clf),\n                                              ('ada_boost', ada_boost_clf),\n                                              ('lgbm', lgbm_clf)],\n                                  voting='hard')\n\n# Train the ensemble model\nensemble_model.fit(X_train, y_train)\n\n# Evaluate the ensemble model\ny_pred = ensemble_model.predict(X_test)\n\n# Model evaluation\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy:', accuracy)\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-06T15:03:10.473774Z","iopub.execute_input":"2024-04-06T15:03:10.474591Z","iopub.status.idle":"2024-04-06T15:03:13.603433Z","shell.execute_reply.started":"2024-04-06T15:03:10.474549Z","shell.execute_reply":"2024-04-06T15:03:13.601070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing necessary libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load the dataset\ndata = pd.read_csv(\"/kaggle/input/australia-weather-data/Weather Training Data.csv\")  # Replace with the actual file path\n\n# Drop columns not needed for prediction\ndata = data.drop(columns=['Evaporation', 'Sunshine'])\n\n# Remove rows with missing values\ndata = data.dropna()\n\n# Encode categorical variables\nle = LabelEncoder()\ndata['Location'] = le.fit_transform(data['Location'])\ndata['WindGustDir'] = le.fit_transform(data['WindGustDir'])\ndata['WindDir9am'] = le.fit_transform(data['WindDir9am'])\ndata['WindDir3pm'] = le.fit_transform(data['WindDir3pm'])\ndata['RainToday'] = le.fit_transform(data['RainToday'])\n\n# Select features and target variable\nX = data[['WindSpeed3pm', 'WindSpeed9am', 'WindGustDir', 'WindDir9am', 'WindDir3pm']]\ny = data['RainToday']\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Model training\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Predicting on the testing set\ny_pred = model.predict(X_test)\n\n# Model evaluation\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-06T15:13:31.008836Z","iopub.execute_input":"2024-04-06T15:13:31.010047Z","iopub.status.idle":"2024-04-06T15:13:37.024417Z","shell.execute_reply.started":"2024-04-06T15:13:31.010008Z","shell.execute_reply":"2024-04-06T15:13:37.023201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing necessary libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load the dataset\ndata = pd.read_csv(\"/kaggle/input/australia-weather-data/Weather Training Data.csv\")  # Replace with the actual file path\n\n# Drop columns not needed for prediction\ndata = data.drop(columns=['Evaporation', 'Sunshine'])\n\n# Remove rows with missing values\ndata = data.dropna()\n\n# Encode categorical variables\nle = LabelEncoder()\ndata['Location'] = le.fit_transform(data['Location'])\ndata['WindGustDir'] = le.fit_transform(data['WindGustDir'])\ndata['WindDir9am'] = le.fit_transform(data['WindDir9am'])\ndata['WindDir3pm'] = le.fit_transform(data['WindDir3pm'])\ndata['RainToday'] = le.fit_transform(data['RainToday'])\n\n# Select features and target variable\nX = data[['WindSpeed3pm', 'WindSpeed9am', 'WindGustDir', 'WindDir9am', 'WindDir3pm']]\ny = data['RainToday']\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define models\nmodels = {\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n    \"Support Vector Machine\": SVC(random_state=42),\n    \"Logistic Regression\": LogisticRegression(random_state=42),\n    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n    \"Gaussian Naive Bayes\": GaussianNB(),\n    \"MLP Classifier\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42),\n    \"XGBoost\": XGBClassifier(random_state=42),\n    \"LightGBM\": LGBMClassifier(random_state=42),\n    # Add more models as needed\n}\n\n# Train and evaluate models\nresults = {}\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    results[name] = accuracy\n\n# Display results\nprint(\"Model\\t\\t\\tAccuracy\")\nprint(\"-\" * 30)\nfor name, accuracy in results.items():\n    print(f\"{name}\\t\\t{accuracy:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-06T15:15:23.270053Z","iopub.execute_input":"2024-04-06T15:15:23.270478Z","iopub.status.idle":"2024-04-06T15:18:17.430620Z","shell.execute_reply.started":"2024-04-06T15:15:23.270446Z","shell.execute_reply":"2024-04-06T15:18:17.429794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w","metadata":{"execution":{"iopub.status.busy":"2024-04-06T16:29:19.903965Z","iopub.execute_input":"2024-04-06T16:29:19.904678Z","iopub.status.idle":"2024-04-06T16:32:09.140097Z","shell.execute_reply.started":"2024-04-06T16:29:19.904643Z","shell.execute_reply":"2024-04-06T16:32:09.138893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing necessary libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n# Load the dataset\ndata = pd.read_csv(\"/kaggle/input/weatherdatabangladesh/Weather_Data.csv\")  # Replace with the actual file path\n\n# Drop columns not needed for prediction\ndata = data.drop(columns=['Evaporation', 'Sunshine', 'Date])\n\n# Remove rows with missing values\ndata = data.dropna()\n\n# Replace values in columns\nreplacement_dict = {\n    'WindGustDir': {'W': 270, 'SE': 135, 'E': 90, 'WSW': 237.5, 'SW': 225, 'SSE': 157.5, 'NNE': 22.5},\n    'WindDir9am': {'W': 270, 'SE': 135, 'E': 90, 'WSW': 237.5, 'SW': 225, 'SSE': 157.5, 'NNE': 22.5},\n    'WindDir3pm': {'W': 270, 'SE': 135, 'E': 90, 'WSW': 237.5, 'SW': 225, 'SSE': 157.5, 'NNE': 22.5}\n}\n\n# Replace the values\nfor column, replacements in replacement_dict.items():\n    data[column].replace(replacements, inplace=True)\n\n# Encode categorical variables\nle = LabelEncoder()\ndata['RainToday'] = le.fit_transform(data['RainToday'])\n\n# Select features and target variable\nX = data.drop(columns=['RainToday'])\ny = data['RainToday']\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define base models\nbase_models = [\n    (\"AdaBoost\", AdaBoostClassifier(random_state=42)),\n    (\"LGBM\", LGBMClassifier(random_state=42)),\n    (\"MLP\", MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)),\n    (\"XGBoost\", XGBClassifier(random_state=42)),\n    (\"SVM\", SVC(random_state=42)),\n    (\"Logistic Regression\", LogisticRegression(random_state=42))\n]\n\n# Create the ensemble model\nensemble_model = VotingClassifier(estimators=base_models, voting='hard')\n\n# Train the ensemble model\nensemble_model.fit(X_train, y_train)\n\n# Evaluate the ensemble model\ny_pred = ensemble_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-06T21:59:51.495808Z","iopub.execute_input":"2024-04-06T21:59:51.496263Z","iopub.status.idle":"2024-04-06T21:59:51.902374Z","shell.execute_reply.started":"2024-04-06T21:59:51.496231Z","shell.execute_reply":"2024-04-06T21:59:51.900427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n# Load the dataset\ndata = pd.read_csv(\"/kaggle/input/weatherdatabangladesh/Weather_Data.csv\")  # Replace with the actual file path\n\n# Drop columns not needed for prediction\ndata = data.drop(columns=['Evaporation', 'Sunshine'])\n\n# Replace values in columns\nreplacement_dict = {\n    'WindGustDir': {'W': 270, 'SE': 135, 'E': 90, 'WSW': 237.5, 'SW': 225, 'SSE': 157.5},\n    'WindDir9am': {'W': 270, 'SE': 135, 'E': 90, 'WSW': 237.5, 'SW': 225, 'SSE': 157.5},\n    'WindDir3pm': {'W': 270, 'SE': 135, 'E': 90, 'WSW': 237.5, 'SW': 225, 'SSE': 157.5}\n}\n\nfor column, replacements in replacement_dict.items():\n    data[column].replace(replacements, inplace=True)\n\n# Treat remaining missing values\nfor column in ['WindGustDir', 'WindDir9am', 'WindDir3pm']:\n    data[column] = pd.to_numeric(data[column], errors='coerce')  # Convert to numeric\n    data[column].fillna(data[column].mean(), inplace=True)  # Replace missing values with mean\n\n# Encode categorical variables\nle = LabelEncoder()\ndata['RainToday'] = le.fit_transform(data['RainToday'])\n\n# Encode 'Location' column with one-hot encoding\ndata = pd.get_dummies(data, columns=['Location'], drop_first=True)\n\n# Select features and target variable\nX = data.drop(columns=['RainToday'])\ny = data['RainToday']\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define base models\nbase_models = [\n    (\"AdaBoost\", AdaBoostClassifier(random_state=42)),\n    (\"LGBM\", LGBMClassifier(random_state=42)),\n    (\"MLP\", MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)),\n    (\"XGBoost\", XGBClassifier(random_state=42)),\n    (\"SVM\", SVC(random_state=42)),\n    (\"Logistic Regression\", LogisticRegression(random_state=42))\n]\n\n# Create the ensemble model\nensemble_model = VotingClassifier(estimators=base_models, voting='hard')\n\n# Train the ensemble model\nensemble_model.fit(X_train, y_train)\n\n# Evaluate the ensemble model\ny_pred = ensemble_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-06T22:04:42.165002Z","iopub.execute_input":"2024-04-06T22:04:42.165517Z","iopub.status.idle":"2024-04-06T22:04:42.357452Z","shell.execute_reply.started":"2024-04-06T22:04:42.165482Z","shell.execute_reply":"2024-04-06T22:04:42.355729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n# Load the dataset\ndata = pd.read_csv(\"/kaggle/input/australia-weather-data/Weather Training Data.csv\")  # Replace with the actual file path\n\n# Drop columns not needed for prediction\ndata = data.drop(columns=['Evaporation', 'Sunshine'])\n\n# Replace values in columns\nreplacement_dict = {\n    'WindGustDir': {'W': 270, 'SE': 135, 'E': 90, 'WSW': 237.5, 'SW': 225, 'SSE': 157.5},\n    'WindDir9am': {'W': 270, 'SE': 135, 'E': 90, 'WSW': 237.5, 'SW': 225, 'SSE': 157.5},\n    'WindDir3pm': {'W': 270, 'SE': 135, 'E': 90, 'WSW': 237.5, 'SW': 225, 'SSE': 157.5}\n}\n\nfor column, replacements in replacement_dict.items():\n    data[column].replace(replacements, inplace=True)\n\n# Treat remaining missing values\nfor column in ['WindGustDir', 'WindDir9am', 'WindDir3pm']:\n    data[column] = pd.to_numeric(data[column], errors='coerce')  # Convert to numeric\n    data[column].fillna(data[column].mean(), inplace=True)  # Replace missing values with mean\n\n# Encode categorical variables\nle = LabelEncoder()\ndata['RainToday'] = le.fit_transform(data['RainToday'])\n\n# Select features and target variable\nX = data.drop(columns=['RainToday'])\ny = data['RainToday']\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define base models\nbase_models = [\n    (\"AdaBoost\", AdaBoostClassifier(random_state=42)),\n    (\"LGBM\", LGBMClassifier(random_state=42)),\n    (\"MLP\", MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)),\n    (\"XGBoost\", XGBClassifier(random_state=42)),\n    (\"SVM\", SVC(random_state=42)),\n    (\"Logistic Regression\", LogisticRegression(random_state=42))\n]\n\n# Create the ensemble model\nensemble_model = VotingClassifier(estimators=base_models, voting='hard')\n\n# Train the ensemble model\nensemble_model.fit(X_train, y_train)\n\n# Evaluate the ensemble model\ny_pred = ensemble_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-06T22:12:39.921172Z","iopub.execute_input":"2024-04-06T22:12:39.921639Z","iopub.status.idle":"2024-04-06T22:12:41.522411Z","shell.execute_reply.started":"2024-04-06T22:12:39.921608Z","shell.execute_reply":"2024-04-06T22:12:41.520443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n# Load the dataset\ndata = pd.read_csv(\"/kaggle/input/weatherdatabangladesh/Weather_Data.csv\")  # Replace with the actual file path\n\n# Drop columns not needed for prediction\ndata = data.drop(columns=['Evaporation', 'Sunshine'])\n\n# Replace values in columns\nreplacement_dict = {\n    'WindGustDir': {'W': 270, 'SE': 135, 'E': 90, 'WSW': 237.5, 'SW': 225, 'SSE': 157.5},\n    'WindDir9am': {'W': 270, 'SE': 135, 'E': 90, 'WSW': 237.5, 'SW': 225, 'SSE': 157.5},\n    'WindDir3pm': {'W': 270, 'SE': 135, 'E': 90, 'WSW': 237.5, 'SW': 225, 'SSE': 157.5}\n}\n\nfor column, replacements in replacement_dict.items():\n    data[column].replace(replacements, inplace=True)\n\n# Treat remaining missing values\nfor column in ['WindGustDir', 'WindDir9am', 'WindDir3pm']:\n    data[column] = pd.to_numeric(data[column], errors='coerce')  # Convert to numeric\n    data[column].fillna(data[column].mean(), inplace=True)  # Replace missing values with mean\n\n# Encode categorical variables\nle = LabelEncoder()\ndata['RainToday'] = le.fit_transform(data['RainToday'])\n\n# Select features and target variable\nX = data.drop(columns=['RainToday'])\ny = data['RainToday']\n\n# Check for NaN values\nprint(\"NaN values in dataset:\", data.isna().sum())\n\n# Drop rows with NaN values\ndata = data.dropna()\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define base models\nbase_models = [\n    (\"AdaBoost\", AdaBoostClassifier(random_state=42)),\n    (\"LGBM\", LGBMClassifier(random_state=42)),\n    (\"MLP\", MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)),\n    (\"XGBoost\", XGBClassifier(random_state=42)),\n    (\"SVM\", SVC(random_state=42)),\n    (\"Logistic Regression\", LogisticRegression(random_state=42))\n]\n\n# Create the ensemble model\nensemble_model = VotingClassifier(estimators=base_models, voting='hard')\n\n# Train the ensemble model\nensemble_model.fit(X_train, y_train)\n\n# Evaluate the ensemble model\ny_pred = ensemble_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T02:43:35.333462Z","iopub.status.idle":"2024-04-07T02:43:35.334065Z","shell.execute_reply.started":"2024-04-07T02:43:35.333821Z","shell.execute_reply":"2024-04-07T02:43:35.333845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n# Load the dataset\ndata = pd.read_csv(\"/kaggle/input/australia-weather-data/Weather Training Data.csv\")  # Replace with the actual file path\n\n# Drop the 'Row_ID' column\ndata.drop(columns=['row ID'], inplace=True)\n\n# Drop columns not needed for prediction\ndata = data.drop(columns=['Evaporation', 'Sunshine', 'Location'])\n\n# Calculate mode for each column\nmode_values = data.mode().iloc[0]\n\n# Replace NA values with mode\ndata.fillna(mode_values, inplace=True)\n\n# Convert categorical columns to numerical using LabelEncoder\nle = LabelEncoder()\nfor column in ['WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']:\n    data[column] = le.fit_transform(data[column])\n\n# Handling categorical countries\n# If the 'Country' column contains categorical values with 5 types, you can encode them using LabelEncoder\n# if 'Country' in data.columns:\n#     data['Country'] = le.fit_transform(data['Country'])\n\n# Select features and target variable\nX = data.drop(columns=['RainToday'])\ny = data['RainToday']\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define base models\nbase_models = [\n    (\"AdaBoost\", AdaBoostClassifier(random_state=42)),\n    (\"LGBM\", LGBMClassifier(random_state=42)),\n    (\"MLP\", MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)),\n    (\"XGBoost\", XGBClassifier(random_state=42)),\n    (\"SVM\", SVC(random_state=42)),\n    (\"Logistic Regression\", LogisticRegression(random_state=42))\n]\n\n# Create the ensemble model\nensemble_model = VotingClassifier(estimators=base_models, voting='hard')\n\n# Train the ensemble model\nensemble_model.fit(X_train, y_train)\n\n# Evaluate the ensemble model\ny_pred = ensemble_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T00:15:25.953724Z","iopub.execute_input":"2024-04-07T00:15:25.954136Z","iopub.status.idle":"2024-04-07T00:27:08.606599Z","shell.execute_reply.started":"2024-04-07T00:15:25.954106Z","shell.execute_reply":"2024-04-07T00:27:08.605223Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 17548, number of negative: 62064\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045863 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2195\n[LightGBM] [Info] Number of data points in the train set: 79612, number of used features: 18\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220419 -> initscore=-1.263226\n[LightGBM] [Info] Start training from score -1.263226\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.9967343247588425\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     15396\n           1       1.00      0.99      0.99      4508\n\n    accuracy                           1.00     19904\n   macro avg       1.00      0.99      1.00     19904\nweighted avg       1.00      1.00      1.00     19904\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import AdaBoostClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n# Load the training and test datasets\ntrain_data = pd.read_csv(\"/kaggle/input/australia-weather-data/Weather Training Data.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/australia-weather-data/Weather Test Data.csv\")\n\n# Drop the 'row ID' column from the training data\ntrain_data.drop(columns=['row ID'], inplace=True)\n\n# Drop columns not needed for prediction\ntrain_data = train_data.drop(columns=['Evaporation', 'Sunshine', 'Location'])\n\n# Calculate mode for each column\nmode_values = train_data.mode().iloc[0]\n\n# Replace NA values with mode in both training and test datasets\ntrain_data.fillna(mode_values, inplace=True)\ntest_data.fillna(mode_values, inplace=True)\n\n# Convert categorical columns to numerical using LabelEncoder\nle = LabelEncoder()\nfor column in ['WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']:\n    train_data[column] = le.fit_transform(train_data[column])\n    test_data[column] = le.transform(test_data[column])\n\n# Select features and target variable in training data\nX_train = train_data.drop(columns=['RainToday'])\ny_train = train_data['RainToday']\n\n# Select features and target variable in test data\nX_test = test_data.drop(columns=['RainToday'])\ny_test_ground_truth = test_data['RainToday']  # Ground truth labels for the test data\n\n# Define base models\nbase_models = [\n    (\"AdaBoost\", AdaBoostClassifier(random_state=42)),\n    (\"LGBM\", LGBMClassifier(random_state=42)),\n    (\"MLP\", MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)),\n    (\"XGBoost\", XGBClassifier(random_state=42)),\n    (\"SVM\", SVC(random_state=42)),\n    (\"Logistic Regression\", LogisticRegression(random_state=42))\n]\n\n# Create the ensemble model\nensemble_model = VotingClassifier(estimators=base_models, voting='hard')\n\n# Train the ensemble model\nensemble_model.fit(X_train, y_train)\n\n# Predict on the test data\ny_pred = ensemble_model.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test_ground_truth, y_pred)\nprint(\"Accuracy:\", accuracy)\nprint(\"\\nClassification Report:\\n\", classification_report(y_test_ground_truth, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T02:11:42.466258Z","iopub.execute_input":"2024-04-07T02:11:42.466802Z","iopub.status.idle":"2024-04-07T02:23:20.494326Z","shell.execute_reply.started":"2024-04-07T02:11:42.466729Z","shell.execute_reply":"2024-04-07T02:23:20.492861Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 22056, number of negative: 77460\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018857 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2200\n[LightGBM] [Info] Number of data points in the train set: 99516, number of used features: 18\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221633 -> initscore=-1.256177\n[LightGBM] [Info] Start training from score -1.256177\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m ensemble_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Predict on the test data\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mensemble_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[1;32m     62\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test_ground_truth, y_pred)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_voting.py:366\u001b[0m, in \u001b[0;36mVotingClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    363\u001b[0m     maj \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_proba(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# 'hard' voting\u001b[39;00m\n\u001b[0;32m--> 366\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m     maj \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mapply_along_axis(\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39margmax(np\u001b[38;5;241m.\u001b[39mbincount(x, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weights_not_none)),\n\u001b[1;32m    369\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    370\u001b[0m         arr\u001b[38;5;241m=\u001b[39mpredictions,\n\u001b[1;32m    371\u001b[0m     )\n\u001b[1;32m    373\u001b[0m maj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_\u001b[38;5;241m.\u001b[39minverse_transform(maj)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_voting.py:68\u001b[0m, in \u001b[0;36m_BaseVoting._predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m     67\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray([est\u001b[38;5;241m.\u001b[39mpredict(X) \u001b[38;5;28;01mfor\u001b[39;00m est \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_])\u001b[38;5;241m.\u001b[39mT\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_voting.py:68\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m     67\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray([\u001b[43mest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m est \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_])\u001b[38;5;241m.\u001b[39mT\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:700\u001b[0m, in \u001b[0;36mAdaBoostClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    684\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict classes for X.\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \n\u001b[1;32m    686\u001b[0m \u001b[38;5;124;03m    The predicted class of an input sample is computed as the weighted mean\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 700\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:761\u001b[0m, in \u001b[0;36mAdaBoostClassifier.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the decision function of ``X``.\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \n\u001b[1;32m    744\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;124;03m    class in ``classes_``, respectively.\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    760\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 761\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    763\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_\n\u001b[1;32m    764\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:94\u001b[0m, in \u001b[0;36mBaseWeightBoosting._check_X\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# Only called to validate X in non-fit methods, therefore reset=False\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:548\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    485\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m    490\u001b[0m ):\n\u001b[1;32m    491\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \n\u001b[1;32m    493\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    552\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    554\u001b[0m         )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:481\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    477\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    479\u001b[0m     )\n\u001b[0;32m--> 481\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n","\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Evaporation\n- Location\n- Sunshine\n- row ID\nFeature names seen at fit time, yet now missing:\n- RainTomorrow\n"],"ename":"ValueError","evalue":"The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Evaporation\n- Location\n- Sunshine\n- row ID\nFeature names seen at fit time, yet now missing:\n- RainTomorrow\n","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import AdaBoostClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n# Load the training and test datasets\ntrain_data = pd.read_csv(\"/kaggle/input/australia-weather-data/Weather Training Data.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/australia-weather-data/Weather Test Data.csv\")\n\n# Drop the 'row ID' column from the training data\ntrain_data.drop(columns=['row ID'], inplace=True)\n\n# Drop columns not needed for prediction\ntrain_data = train_data.drop(columns=['Evaporation', 'Sunshine', 'Location'])\n\n# Calculate mode for each column in the training data\nmode_values = train_data.mode().iloc[0]\n\n# Replace NA values with mode in both training and test datasets\ntrain_data.fillna(mode_values, inplace=True)\ntest_data.fillna(mode_values, inplace=True)\n\n# Convert categorical columns to numerical using LabelEncoder\nle = LabelEncoder()\nfor column in ['WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']:\n    train_data[column] = le.fit_transform(train_data[column])\n    test_data[column] = le.transform(test_data[column])\n\n# Select features and target variable in training data\nX_train = train_data.drop(columns=['RainToday'])\ny_train = train_data['RainToday']\n\n# Select features and target variable in test data\nX_test = test_data.drop(columns=['RainToday'])\ny_test_ground_truth = test_data['RainToday']  # Ground truth labels for the test data\n\n# Define base models\nbase_models = [\n    (\"AdaBoost\", AdaBoostClassifier(random_state=42)),\n    (\"LGBM\", LGBMClassifier(random_state=42)),\n    (\"MLP\", MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)),\n    (\"XGBoost\", XGBClassifier(random_state=42)),\n    (\"SVM\", SVC(random_state=42)),\n    (\"Logistic Regression\", LogisticRegression(random_state=42))\n]\n\n# Create the ensemble model\nensemble_model = VotingClassifier(estimators=base_models, voting='hard')\n\n# Train the ensemble model\nensemble_model.fit(X_train, y_train)\n\n# Predict on the test data\ny_pred = ensemble_model.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test_ground_truth, y_pred)\nprint(\"Accuracy:\", accuracy)\nprint(\"\\nClassification Report:\\n\", classification_report(y_test_ground_truth, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T02:32:50.079885Z","iopub.execute_input":"2024-04-07T02:32:50.080397Z","iopub.status.idle":"2024-04-07T02:43:35.327946Z","shell.execute_reply.started":"2024-04-07T02:32:50.080361Z","shell.execute_reply":"2024-04-07T02:43:35.325401Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 22056, number of negative: 77460\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023846 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2200\n[LightGBM] [Info] Number of data points in the train set: 99516, number of used features: 18\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221633 -> initscore=-1.256177\n[LightGBM] [Info] Start training from score -1.256177\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m ensemble_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Predict on the test data\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mensemble_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[1;32m     62\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test_ground_truth, y_pred)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_voting.py:366\u001b[0m, in \u001b[0;36mVotingClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    363\u001b[0m     maj \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_proba(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# 'hard' voting\u001b[39;00m\n\u001b[0;32m--> 366\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m     maj \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mapply_along_axis(\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39margmax(np\u001b[38;5;241m.\u001b[39mbincount(x, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weights_not_none)),\n\u001b[1;32m    369\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    370\u001b[0m         arr\u001b[38;5;241m=\u001b[39mpredictions,\n\u001b[1;32m    371\u001b[0m     )\n\u001b[1;32m    373\u001b[0m maj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_\u001b[38;5;241m.\u001b[39minverse_transform(maj)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_voting.py:68\u001b[0m, in \u001b[0;36m_BaseVoting._predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m     67\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray([est\u001b[38;5;241m.\u001b[39mpredict(X) \u001b[38;5;28;01mfor\u001b[39;00m est \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_])\u001b[38;5;241m.\u001b[39mT\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_voting.py:68\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m     67\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray([\u001b[43mest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m est \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_])\u001b[38;5;241m.\u001b[39mT\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:700\u001b[0m, in \u001b[0;36mAdaBoostClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    684\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict classes for X.\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \n\u001b[1;32m    686\u001b[0m \u001b[38;5;124;03m    The predicted class of an input sample is computed as the weighted mean\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 700\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:761\u001b[0m, in \u001b[0;36mAdaBoostClassifier.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the decision function of ``X``.\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \n\u001b[1;32m    744\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;124;03m    class in ``classes_``, respectively.\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    760\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 761\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    763\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_\n\u001b[1;32m    764\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:94\u001b[0m, in \u001b[0;36mBaseWeightBoosting._check_X\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# Only called to validate X in non-fit methods, therefore reset=False\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:548\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    485\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m    490\u001b[0m ):\n\u001b[1;32m    491\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \n\u001b[1;32m    493\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    552\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    554\u001b[0m         )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:481\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    477\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    479\u001b[0m     )\n\u001b[0;32m--> 481\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n","\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Evaporation\n- Location\n- Sunshine\n- row ID\nFeature names seen at fit time, yet now missing:\n- RainTomorrow\n"],"ename":"ValueError","evalue":"The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Evaporation\n- Location\n- Sunshine\n- row ID\nFeature names seen at fit time, yet now missing:\n- RainTomorrow\n","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.ensemble import AdaBoostClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.impute import SimpleImputer\n\n# Load the training and test datasets\ntrain_data = pd.read_csv(\"/kaggle/input/australia-weather-data/Weather Training Data.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/australia-weather-data/Weather Test Data.csv\")\n\n# Drop unnecessary columns\ndrop_columns = ['Evaporation', 'Sunshine', 'Location', 'row ID']\ntrain_data.drop(columns=drop_columns, inplace=True)\ntest_data.drop(columns=drop_columns, inplace=True)\n\n# Handle missing values\nimputer = SimpleImputer(strategy='most_frequent')\ntrain_data = pd.DataFrame(imputer.fit_transform(train_data), columns=train_data.columns)\ntest_data = pd.DataFrame(imputer.transform(test_data), columns=test_data.columns)\n\n# Convert categorical columns to numerical using LabelEncoder\nle = LabelEncoder()\nfor column in ['WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']:\n    train_data[column] = le.fit_transform(train_data[column])\n    test_data[column] = le.transform(test_data[column])\n\n# Select features and target variable in training data\nX_train = train_data.drop(columns=['RainToday'])\ny_train = train_data['RainToday']\n\n# Select features and target variable in test data\nX_test = test_data.drop(columns=['RainToday'])\ny_test_ground_truth = test_data['RainToday']  # Ground truth labels for the test data\n\n# Scale numerical features\nscaler = StandardScaler()\nnumerical_columns = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm',\n                     'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm',\n                     'Temp9am', 'Temp3pm']\nX_train[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])\nX_test[numerical_columns] = scaler.transform(X_test[numerical_columns])\n\n# Define base models\nbase_models = [\n    (\"AdaBoost\", AdaBoostClassifier(random_state=42)),\n    (\"LGBM\", LGBMClassifier(random_state=42)),\n    (\"MLP\", MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)),\n    (\"XGBoost\", XGBClassifier(random_state=42)),\n    (\"SVM\", SVC(random_state=42)),\n    (\"Logistic Regression\", LogisticRegression(max_iter=1000, random_state=42))\n]\n\n# Create the ensemble model\nensemble_model = VotingClassifier(estimators=base_models, voting='hard')\n\n# Train the ensemble model\nensemble_model.fit(X_train, y_train)\n\n# Predict on the test data\ny_pred = ensemble_model.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test_ground_truth, y_pred)\nprint(\"Accuracy:\", accuracy)\nprint(\"\\nClassification Report:\\n\", classification_report(y_test_ground_truth, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T02:47:58.945675Z","iopub.execute_input":"2024-04-07T02:47:58.946187Z","iopub.status.idle":"2024-04-07T02:48:00.883912Z","shell.execute_reply.started":"2024-04-07T02:47:58.946145Z","shell.execute_reply":"2024-04-07T02:48:00.881918Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m imputer \u001b[38;5;241m=\u001b[39m SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmost_frequent\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m train_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(imputer\u001b[38;5;241m.\u001b[39mfit_transform(train_data), columns\u001b[38;5;241m=\u001b[39mtrain_data\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m---> 24\u001b[0m test_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mimputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39mtest_data\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Convert categorical columns to numerical using LabelEncoder\u001b[39;00m\n\u001b[1;32m     27\u001b[0m le \u001b[38;5;241m=\u001b[39m LabelEncoder()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/impute/_base.py:551\u001b[0m, in \u001b[0;36mSimpleImputer.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Impute all missing values in `X`.\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \n\u001b[1;32m    538\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;124;03m    `X` with imputed values.\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    549\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 551\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m statistics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatistics_\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m statistics\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/impute/_base.py:344\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[0;34m(self, X, in_fit)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m new_ve \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ve\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_fit:\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;66;03m# Use the dtype seen in `fit` for non-`fit` conversion\u001b[39;00m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_dtype \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdtype\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/impute/_base.py:327\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[0;34m(self, X, in_fit)\u001b[0m\n\u001b[1;32m    324\u001b[0m     force_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_fit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not convert\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ve):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:548\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    485\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m    490\u001b[0m ):\n\u001b[1;32m    491\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \n\u001b[1;32m    493\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    552\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    554\u001b[0m         )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:481\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    477\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    479\u001b[0m     )\n\u001b[0;32m--> 481\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n","\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- RainTomorrow\n"],"ename":"ValueError","evalue":"The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- RainTomorrow\n","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n# Load the dataset\ndata = pd.read_csv(\"/kaggle/input/australia-weather-data/Weather Test Data.csv\")  # Replace with the actual file path\n\n# Drop the 'Row_ID' column\ndata.drop(columns=['row ID'], inplace=True)\n\n# Drop columns not needed for prediction\ndata = data.drop(columns=['Evaporation', 'Sunshine', 'Location'])\n\n# Calculate mode for each column\nmode_values = data.mode().iloc[0]\n\n# Replace NA values with mode\ndata.fillna(mode_values, inplace=True)\n\n# Convert categorical columns to numerical using LabelEncoder\nle = LabelEncoder()\nfor column in ['WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']:\n    data[column] = le.fit_transform(data[column])\n\n# Handling categorical countries\n# If the 'Country' column contains categorical values with 5 types, you can encode them using LabelEncoder\n# if 'Country' in data.columns:\n#     data['Country'] = le.fit_transform(data['Country'])\n\n# Select features and target variable\nX = data.drop(columns=['RainToday'])\ny = data['RainToday']\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define base models\nbase_models = [\n    (\"AdaBoost\", AdaBoostClassifier(random_state=42)),\n    (\"LGBM\", LGBMClassifier(random_state=42)),\n    (\"MLP\", MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)),\n    (\"XGBoost\", XGBClassifier(random_state=42)),\n    (\"SVM\", SVC(random_state=42)),\n    (\"Logistic Regression\", LogisticRegression(random_state=42))\n]\n\n# Create the ensemble model\nensemble_model = VotingClassifier(estimators=base_models, voting='hard')\n\n# Train the ensemble model\nensemble_model.fit(X_train, y_train)\n\n# Evaluate the ensemble model\ny_pred = ensemble_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy*100)\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T02:55:04.930807Z","iopub.execute_input":"2024-04-07T02:55:04.934088Z","iopub.status.idle":"2024-04-07T02:56:02.280495Z","shell.execute_reply.started":"2024-04-07T02:55:04.934030Z","shell.execute_reply":"2024-04-07T02:56:02.278927Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 7458, number of negative: 26683\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019250 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2179\n[LightGBM] [Info] Number of data points in the train set: 34141, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.218447 -> initscore=-1.274739\n[LightGBM] [Info] Start training from score -1.274739\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 100.0\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00      6595\n           1       1.00      1.00      1.00      1941\n\n    accuracy                           1.00      8536\n   macro avg       1.00      1.00      1.00      8536\nweighted avg       1.00      1.00      1.00      8536\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n# Load the dataset\ndata = pd.read_csv(\"/kaggle/input/australia-weather-data/Weather Training Data.csv\")  # Replace with the actual file path\n\n# Trim the dataset to 10,000 rows\ndata = data.head(70000)\n\n# Drop the 'row ID' column\ndata.drop(columns=['row ID'], inplace=True)\n\n# Drop columns not needed for prediction\ndata = data.drop(columns=['Evaporation', 'Sunshine', 'Location'])\n\n# Calculate mode for each column\nmode_values = data.mode().iloc[0]\n\n# Replace NA values with mode\ndata.fillna(mode_values, inplace=True)\n\n# Convert categorical columns to numerical using LabelEncoder\nle = LabelEncoder()\nfor column in ['WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']:\n    data[column] = le.fit_transform(data[column])\n\n# Select features and target variable\nX = data.drop(columns=['RainToday'])\ny = data['RainToday']\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define base models\nbase_models = [\n#     (\"AdaBoost\", AdaBoostClassifier(random_state=42)),\n#     (\"LGBM\", LGBMClassifier(random_state=42)),\n#     (\"MLP\", MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)),\n#     (\"XGBoost\", XGBClassifier(random_state=42)),\n#     (\"SVM\", SVC(random_state=42)),\n    (\"Logistic Regression\", LogisticRegression(random_state=42))\n]\n\n# Create the ensemble model\nensemble_model = VotingClassifier(estimators=base_models, voting='hard')\n\n# Train the ensemble model\nensemble_model.fit(X_train, y_train)\n\n# Evaluate the ensemble model\ny_pred = ensemble_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T03:16:00.904591Z","iopub.execute_input":"2024-04-07T03:16:00.905346Z","iopub.status.idle":"2024-04-07T03:16:02.724540Z","shell.execute_reply.started":"2024-04-07T03:16:00.905291Z","shell.execute_reply":"2024-04-07T03:16:02.721659Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.9857142857142858\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99     10876\n           1       0.98      0.96      0.97      3124\n\n    accuracy                           0.99     14000\n   macro avg       0.98      0.98      0.98     14000\nweighted avg       0.99      0.99      0.99     14000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the dataset\ndata = pd.read_csv(\"/kaggle/input/australia-weather-data/Weather Training Data.csv\")  # Replace with the actual file path\n\n# Trim the dataset to 10,000 rows\ndata = data.head(10000)\n\n# Drop the 'row ID' column\ndata.drop(columns=['row ID'], inplace=True)\n\n# Drop columns not needed for prediction\ndata = data.drop(columns=['Evaporation', 'Sunshine', 'Location'])\n\n# Calculate mode for each column\nmode_values = data.mode().iloc[0]\n\n# Replace NA values with mode\ndata.fillna(mode_values, inplace=True)\n\n# Convert categorical columns to numerical using LabelEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nfor column in ['WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']:\n    data[column] = le.fit_transform(data[column])\n\n# Select features and target variable\nX = data.drop(columns=['RainToday'])\ny = data['RainToday']\n\n# Implement the Find-S algorithm\ndef find_s_algorithm(X, y):\n    hypothesis = None\n    for i in range(len(X)):\n        if y[i] == 1:\n            if hypothesis is None:\n                hypothesis = X.iloc[i]\n            else:\n                for j in range(len(X.columns)):\n                    if hypothesis[j] != X.iloc[i][j]:\n                        hypothesis[j] = '?'\n    return hypothesis\n\n# Apply the Find-S algorithm\nhypothesis = find_s_algorithm(X, y)\n\n# Predict using the hypothesis\ny_pred = []\nfor i in range(len(X)):\n    y_pred.append(1 if all(hypothesis[j] == X.iloc[i][j] or hypothesis[j] == '?' for j in range(len(X.columns))) else 0)\n\n# Evaluate the model\nfrom sklearn.metrics import accuracy_score, classification_report\n\naccuracy = accuracy_score(y, y_pred)\nprint(\"Accuracy:\", accuracy)\nprint(\"\\nClassification Report:\\n\", classification_report(y, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T03:34:40.131832Z","iopub.execute_input":"2024-04-07T03:34:40.133171Z","iopub.status.idle":"2024-04-07T03:35:18.381020Z","shell.execute_reply.started":"2024-04-07T03:34:40.133130Z","shell.execute_reply":"2024-04-07T03:35:18.379726Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_33/3232020306.py:41: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  if hypothesis[j] != X.iloc[i][j]:\n/tmp/ipykernel_33/3232020306.py:42: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  hypothesis[j] = '?'\n/tmp/ipykernel_33/3232020306.py:42: FutureWarning: Series.__setitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To set a value by position, use `ser.iloc[pos] = value`\n  hypothesis[j] = '?'\n/tmp/ipykernel_33/3232020306.py:42: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '?' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  hypothesis[j] = '?'\n/tmp/ipykernel_33/3232020306.py:51: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  y_pred.append(1 if all(hypothesis[j] == X.iloc[i][j] or hypothesis[j] == '?' for j in range(len(X.columns))) else 0)\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.1936\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00      8064\n           1       0.19      1.00      0.32      1936\n\n    accuracy                           0.19     10000\n   macro avg       0.10      0.50      0.16     10000\nweighted avg       0.04      0.19      0.06     10000\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Load the dataset\ndata = pd.read_csv(\"/kaggle/input/australia-weather-data/Weather Training Data.csv\")  # Replace with the actual file path\n\n# Trim the dataset to 10,000 rows\ndata = data.head(10000)\n\n# Drop the 'row ID' column\ndata.drop(columns=['row ID'], inplace=True)\n\n# Drop columns not needed for prediction\ndata = data.drop(columns=['Evaporation', 'Sunshine', 'Location'])\n\n# Calculate mode for each column\nmode_values = data.mode().iloc[0]\n\n# Replace NA values with mode\ndata.fillna(mode_values, inplace=True)\n\n# Convert categorical columns to numerical using LabelEncoder\nle = LabelEncoder()\nfor column in ['WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']:\n    data[column] = le.fit_transform(data[column])\n\n# Select features and target variable\nX = data.drop(columns=['RainToday'])\ny = data['RainToday']\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define base models\nbase_models = [\n    (\"Decision Tree\", DecisionTreeClassifier(random_state=42)),\n    (\"KNN\", KNeighborsClassifier()),\n    (\"Naive Bayes\", GaussianNB()),\n    (\"Random Forest\", RandomForestClassifier(random_state=42))\n]\n\n# Evaluate each model\nfor name, model in base_models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"{name} Accuracy:\", accuracy)\n    print(f\"\\n{name} Classification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T03:36:34.627105Z","iopub.execute_input":"2024-04-07T03:36:34.627621Z","iopub.status.idle":"2024-04-07T03:36:36.310125Z","shell.execute_reply.started":"2024-04-07T03:36:34.627585Z","shell.execute_reply":"2024-04-07T03:36:36.308685Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Decision Tree Accuracy: 1.0\n\nDecision Tree Classification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00      1595\n           1       1.00      1.00      1.00       405\n\n    accuracy                           1.00      2000\n   macro avg       1.00      1.00      1.00      2000\nweighted avg       1.00      1.00      1.00      2000\n\nKNN Accuracy: 0.8905\n\nKNN Classification Report:\n               precision    recall  f1-score   support\n\n           0       0.90      0.97      0.93      1595\n           1       0.83      0.57      0.68       405\n\n    accuracy                           0.89      2000\n   macro avg       0.87      0.77      0.81      2000\nweighted avg       0.89      0.89      0.88      2000\n\nNaive Bayes Accuracy: 0.9565\n\nNaive Bayes Classification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      0.95      0.97      1595\n           1       0.82      1.00      0.90       405\n\n    accuracy                           0.96      2000\n   macro avg       0.91      0.97      0.94      2000\nweighted avg       0.96      0.96      0.96      2000\n\nRandom Forest Accuracy: 1.0\n\nRandom Forest Classification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00      1595\n           1       1.00      1.00      1.00       405\n\n    accuracy                           1.00      2000\n   macro avg       1.00      1.00      1.00      2000\nweighted avg       1.00      1.00      1.00      2000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Load the dataset\ndata = pd.read_csv(\"/kaggle/input/australia-weather-data/Weather Training Data.csv\")  # Replace with the actual file path\n\n# Trim the dataset to 10,000 rows\n# data = data.head(90000)\n\n# Drop the 'row ID' column\ndata.drop(columns=['row ID'], inplace=True)\n\n# Drop columns not needed for prediction\ndata = data.drop(columns=['Evaporation', 'Sunshine', 'Location'])\n\n# Calculate mode for each column\nmode_values = data.mode().iloc[0]\n\n# Replace NA values with mode\ndata.fillna(mode_values, inplace=True)\n\n# Convert categorical columns to numerical using LabelEncoder\nle = LabelEncoder()\nfor column in ['WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']:\n    data[column] = le.fit_transform(data[column])\n\n# Select features and target variable\nX = data.drop(columns=['RainToday'])\ny = data['RainToday']\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define base models\nbase_models = [\n    (\"Logistic Regression\", LogisticRegression(random_state=42)),\n    (\"KNN\", KNeighborsClassifier()),\n    (\"Decision Tree\", DecisionTreeClassifier(random_state=42))\n]\n\n# Evaluate each model\nfor name, model in base_models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"{name} Accuracy:\", accuracy)\n    print(f\"\\n{name} Classification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T03:40:23.166750Z","iopub.execute_input":"2024-04-07T03:40:23.167626Z","iopub.status.idle":"2024-04-07T03:40:33.925095Z","shell.execute_reply.started":"2024-04-07T03:40:23.167585Z","shell.execute_reply":"2024-04-07T03:40:33.924130Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression Accuracy: 0.9838888888888889\n\nLogistic Regression Classification Report:\n               precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99     13825\n           1       0.98      0.95      0.96      4175\n\n    accuracy                           0.98     18000\n   macro avg       0.98      0.97      0.98     18000\nweighted avg       0.98      0.98      0.98     18000\n\nKNN Accuracy: 0.8832777777777778\n\nKNN Classification Report:\n               precision    recall  f1-score   support\n\n           0       0.89      0.97      0.93     13825\n           1       0.84      0.61      0.71      4175\n\n    accuracy                           0.88     18000\n   macro avg       0.87      0.79      0.82     18000\nweighted avg       0.88      0.88      0.88     18000\n\nDecision Tree Accuracy: 1.0\n\nDecision Tree Classification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     13825\n           1       1.00      1.00      1.00      4175\n\n    accuracy                           1.00     18000\n   macro avg       1.00      1.00      1.00     18000\nweighted avg       1.00      1.00      1.00     18000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LinearRegression\n\n# Load the dataset\ndata = pd.read_csv(\"/kaggle/input/australia-weather-data/Weather Training Data.csv\")  # Replace with the actual file path\n\n# Trim the dataset to 10,000 rows\n# data = data.head(90000)\n\n# Drop the 'row ID' column\ndata.drop(columns=['row ID'], inplace=True)\n\n# Drop columns not needed for prediction\ndata = data.drop(columns=['Evaporation', 'Sunshine', 'Location'])\n\n# Calculate mode for each column\nmode_values = data.mode().iloc[0]\n\n# Replace NA values with mode\ndata.fillna(mode_values, inplace=True)\n\n# Convert categorical columns to numerical using LabelEncoder\nle = LabelEncoder()\nfor column in ['WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']:\n    data[column] = le.fit_transform(data[column])\n\n# Select features and target variable\nX = data.drop(columns=['RainToday'])\ny = data['RainToday']\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define base models\nbase_models = [\n    (\"Logistic Regression\", LogisticRegression(random_state=42)),\n    (\"KNN\", KNeighborsClassifier()),\n    (\"Decision Tree\", DecisionTreeClassifier(random_state=42)),\n    (\"Linear Regression\", LinearRegression())\n]\n\n# Evaluate each model\nfor name, model in base_models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    # Convert regression predictions to binary classes for comparison\n    y_pred_binary = [1 if pred >= 0.5 else 0 for pred in y_pred]\n    accuracy = accuracy_score(y_test, y_pred_binary)\n    print(f\"{name} Accuracy:\", accuracy)\n    print(f\"\\n{name} Classification Report:\\n\", classification_report(y_test, y_pred_binary))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T03:48:11.776197Z","iopub.execute_input":"2024-04-07T03:48:11.778510Z","iopub.status.idle":"2024-04-07T03:48:25.267629Z","shell.execute_reply.started":"2024-04-07T03:48:11.778440Z","shell.execute_reply":"2024-04-07T03:48:25.266652Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression Accuracy: 0.9891981511254019\n\nLogistic Regression Classification Report:\n               precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99     15396\n           1       0.98      0.97      0.98      4508\n\n    accuracy                           0.99     19904\n   macro avg       0.99      0.98      0.98     19904\nweighted avg       0.99      0.99      0.99     19904\n\nKNN Accuracy: 0.8869573954983923\n\nKNN Classification Report:\n               precision    recall  f1-score   support\n\n           0       0.90      0.97      0.93     15396\n           1       0.84      0.62      0.71      4508\n\n    accuracy                           0.89     19904\n   macro avg       0.87      0.79      0.82     19904\nweighted avg       0.88      0.89      0.88     19904\n\nDecision Tree Accuracy: 1.0\n\nDecision Tree Classification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     15396\n           1       1.00      1.00      1.00      4508\n\n    accuracy                           1.00     19904\n   macro avg       1.00      1.00      1.00     19904\nweighted avg       1.00      1.00      1.00     19904\n\nLinear Regression Accuracy: 0.8605305466237942\n\nLinear Regression Classification Report:\n               precision    recall  f1-score   support\n\n           0       0.85      0.99      0.92     15396\n           1       0.93      0.41      0.57      4508\n\n    accuracy                           0.86     19904\n   macro avg       0.89      0.70      0.74     19904\nweighted avg       0.87      0.86      0.84     19904\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\n# Load the dataset\ndata = pd.read_csv(\"/kaggle/input/australia-weather-data/Weather Training Data.csv\")  # Replace with the actual file path\n\n# Trim the dataset to 10,000 rows\n# data = data.head(90000)\n\n# Drop the 'row ID' column\ndata.drop(columns=['row ID'], inplace=True)\n\n# Drop columns not needed for prediction\ndata = data.drop(columns=['Evaporation', 'Sunshine', 'Location'])\n\n# Calculate mode for each column\nmode_values = data.mode().iloc[0]\n\n# Replace NA values with mode\ndata.fillna(mode_values, inplace=True)\n\n# Convert categorical columns to numerical using LabelEncoder\nle = LabelEncoder()\nfor column in ['WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']:\n    data[column] = le.fit_transform(data[column])\n\n# Select features and target variable\nX = data.drop(columns=['RainToday'])\ny = data['RainToday']\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define base models\nbase_models = [\n    (\"Logistic Regression\", LogisticRegression(random_state=42)),\n    (\"KNN\", KNeighborsClassifier()),\n    (\"Decision Tree\", DecisionTreeClassifier(random_state=42)),\n    (\"Naive Bayes\", GaussianNB()),\n    (\"SVM\", SVC(random_state=42)),\n    (\"Random Forest\", RandomForestClassifier(random_state=42)),\n    (\"Gradient Boosting\", GradientBoostingClassifier(random_state=42))\n]\n\n# Evaluate each model\nfor name, model in base_models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"{name} Accuracy:\", accuracy)\n    print(f\"\\n{name} Classification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T03:50:31.007411Z","iopub.execute_input":"2024-04-07T03:50:31.007930Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression Accuracy: 0.9891981511254019\n\nLogistic Regression Classification Report:\n               precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99     15396\n           1       0.98      0.97      0.98      4508\n\n    accuracy                           0.99     19904\n   macro avg       0.99      0.98      0.98     19904\nweighted avg       0.99      0.99      0.99     19904\n\nKNN Accuracy: 0.8869573954983923\n\nKNN Classification Report:\n               precision    recall  f1-score   support\n\n           0       0.90      0.97      0.93     15396\n           1       0.84      0.62      0.71      4508\n\n    accuracy                           0.89     19904\n   macro avg       0.87      0.79      0.82     19904\nweighted avg       0.88      0.89      0.88     19904\n\nDecision Tree Accuracy: 1.0\n\nDecision Tree Classification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     15396\n           1       1.00      1.00      1.00      4508\n\n    accuracy                           1.00     19904\n   macro avg       1.00      1.00      1.00     19904\nweighted avg       1.00      1.00      1.00     19904\n\nNaive Bayes Accuracy: 0.9538283762057878\n\nNaive Bayes Classification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      0.94      0.97     15396\n           1       0.83      1.00      0.91      4508\n\n    accuracy                           0.95     19904\n   macro avg       0.92      0.97      0.94     19904\nweighted avg       0.96      0.95      0.96     19904\n\n","output_type":"stream"}]}]}